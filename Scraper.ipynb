{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fac1c1e",
   "metadata": {},
   "source": [
    "# Web Scraping Script Documentation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Web scraping is the process of extracting content and data from a website using scripts or bots. Web scraping allows you to obtain the underlying HTML code and, with it, data from a database. The scraper can then copy the complete website content (or acquired information) elsewhere in a more user-friendly style. Be it a spreadsheet or an API.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "\n",
    "This web scraper script in Python is created as part of the first Task in the internship program, following the requirements below:\n",
    "\n",
    "1. **Website Selection:**\n",
    "   - This script is designed to scrape data from the Junkbooks website (https://www.junkybooks.com/), a publicly accessible source of many different books in PDF format. \n",
    "\n",
    "\n",
    "2. **Web Scraping:**\n",
    "   - Utilizing the Beautiful Soup and Requests libraries, the script extracts the names of all the books available on the website with their corresponding downloading links.\n",
    "\n",
    "\n",
    "3. **Data Processing:**\n",
    "   - The script extracts and displays the names of all the books available on the website including their corresponding downloading links.\n",
    "\n",
    "\n",
    "4. **Automation:**\n",
    "   - The script is automated by scheduling it to run every 24 hours. This helps keep the dataset up-to-date.\n",
    "\n",
    "\n",
    "5. **Documentation:**\n",
    "   - Documentation that explains the purpose of the script, how to execute it, dependencies used, and additional notes is added to the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d999fea",
   "metadata": {},
   "source": [
    "### 1. Import the necessary livraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2420ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import schedule\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246363c8",
   "metadata": {},
   "source": [
    "## Imported Libraries\n",
    "\n",
    "1. **requests**: is an HTTP library that allows to send HTTP/1.1 requests extremely easily. It is commonly employed to fetch data from websites.\n",
    "\n",
    "2. **BeautifulSoup**: used to make it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "\n",
    "3. **schedule**: A simple to use API for scheduling tasks. Enables the automation of recurring tasks at specified intervals.\n",
    "\n",
    "4. **time**: Is used for introducing delays in the script. It can be employed to control the timing of various operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a648b13",
   "metadata": {},
   "source": [
    "### 2. Define url of the website to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b3b2de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "_baseurl_ = \"https://www.junkybooks.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff405457",
   "metadata": {},
   "source": [
    "### 3. Define empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "153e1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_titles = []\n",
    "download_pages = []\n",
    "download_links = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44bd54c",
   "metadata": {},
   "source": [
    "These lists are designed to hold the following data respectively.\n",
    "\n",
    "1. **book_titles:** Holds the titles of all the books listed on the website.\n",
    "2. **download_pages:** holds the web addresses where each book can be found.\n",
    "3. **download_links:** Holds the web address from which each book is downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125fc6a",
   "metadata": {},
   "source": [
    "### 4. Define the first webscrap Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c71d101",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scrape_web():\n",
    "    index = 1\n",
    "    while True:\n",
    "        link = \"https://www.junkybooks.com/books?page=\"+str(index)\n",
    "        page = requests.get(link)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        results = soup.find(\"div\", class_=\"tab-content\")\n",
    "        if results is not None:\n",
    "            books = results.find_all(\"div\", class_=\"product-details text-center\")\n",
    "        else:\n",
    "            print(f\"Parsing stops here since END OF WEB PAGES is reached\")\n",
    "            scrape_webpage(download_pages)\n",
    "            break\n",
    "        for book in books:\n",
    "            title = book.find(\"h4\")\n",
    "            title_text = title.get_text()\n",
    "            book_titles.append(title_text)\n",
    "            download_path = title.find('a')['href'].strip()\n",
    "            download_link = \"https://www.junkybooks.com/\" + download_path\n",
    "            download_pages.append(download_link)\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcab4d",
   "metadata": {},
   "source": [
    "This (scrape_web) function is defined to facilitate web scraping. It sets the initial URL, performs scraping each webpage (till end of the page), then finds the title and url of the download page for each book on the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448e6f1",
   "metadata": {},
   "source": [
    "### 5. Define the second webscrap Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "131cd2c0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scrape_webpage(download_pages):\n",
    "    for download_page in download_pages:\n",
    "        download_url = requests.get(download_page)\n",
    "        soup = BeautifulSoup(download_url.content, \"html.parser\")\n",
    "\n",
    "        results = soup.find(\"div\", class_=\"product-info-main\")\n",
    "        books = results.find_all(\"div\", class_=\"product-add-form\")\n",
    "        for book in books:\n",
    "            download_button = book.find(\"form\")\n",
    "            download_subDir = download_button.find('a')['href'].strip()\n",
    "            download_link = \"https://www.junkybooks.com/\" + download_subDir.strip(\"..?\")\n",
    "            download_links.append(download_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116667d",
   "metadata": {},
   "source": [
    "This second webscraper function (scrape_webpage) is defined to facilitate webscraping based on the download pages recieved from the first scraper function (scrape_webp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd841dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_web()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9212ffe",
   "metadata": {},
   "source": [
    "### 6. Print the scraped data (title of each book with their corresponding download link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "689bb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0\n",
    "if download_links is not None:\n",
    "    for n, download_link in enumerate(download_links):\n",
    "        print(f\"===========================================================================================================================================||\")\n",
    "        print(f\"||#{n} BOOK TITLE: ||  {book_titles[n]}\")\n",
    "        print(f\"-------------------------------------------------------------------------------------------------------------------------------------------||\")\n",
    "        print(f\"||DOWNLOAD LINK:  || {download_link}\")\n",
    "        m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88304d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================||\n",
      "Total of 0 Books Scraped from the website\n",
      "============================================================================================================================================||\n"
     ]
    }
   ],
   "source": [
    "print(f\"============================================================================================================================================||\")\n",
    "print(f\"Total of {m} Books Scraped from the website\")\n",
    "print(f\"============================================================================================================================================||\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff5d39",
   "metadata": {},
   "source": [
    "### 7. Schedule the task to run every 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85312774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schedule the task to run every 24 hours\n",
    "schedule.every(2).seconds.do(scrape_web)\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
